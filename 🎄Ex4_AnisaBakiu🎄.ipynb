{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    font-family: 'Old English Text MT', 'Goudy Bookletter 1911', serif;\n",
    "    font-size: 38px;\n",
    "    font-weight: bold;\n",
    "    letter-spacing: 2px;\n",
    "    color:rgb(197, 56, 37);\n",
    "    text-align: center;\n",
    "    margin-bottom: 20px;\n",
    "\">\n",
    "Esercitazione IV\n",
    "</div>\n",
    "\n",
    "<div style=\"\n",
    "    font-family: 'Goudy Bookletter 1911', 'Old English Text MT', 'Times New Roman', serif;\n",
    "    font-size: 18px;\n",
    "    letter-spacing: 1.5px;\n",
    "    line-height: 1.4;\n",
    "    color:rgb(244, 105, 46); /* Goldenrod */\n",
    "    text-align: center;\n",
    "\">\n",
    "Addestra un LSTM a generare testo nello stile di Shakespeare, prevedendo un carattere alla volta.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ____  _           _                                       \n",
      "/ ___|| |__   __ _| | _____  ___ _ __   ___  __ _ _ __ ___ \n",
      "\\___ \\| '_ \\ / _` | |/ / _ \\/ __| '_ \\ / _ \\/ _` | '__/ _ \\\n",
      " ___) | | | | (_| |   <  __/\\__ \\ |_) |  __/ (_| | | |  __/\n",
      "|____/|_| |_|\\__,_|_|\\_\\___||___/ .__/ \\___|\\__,_|_|  \\___|\n",
      "                                |_|                        \n",
      " _     ____ _____ __  __ \n",
      "| |   / ___|_   _|  \\/  |\n",
      "| |   \\___ \\ | | | |\\/| |\n",
      "| |___ ___) || | | |  | |\n",
      "|_____|____/ |_| |_|  |_|\n",
      "                         \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyfiglet\n",
    "\n",
    "ascii_text = pyfiglet.figlet_format(\"Shakespeare LSTM\")\n",
    "print(ascii_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per addestrare la rete LSTM a generare testo a livello di carattere è necessario disporre di un corpus sufficientemente lungo, però lo stile di Shakespeare è ben definito e facilmente riconoscibile.\n",
    "\n",
    "Il testo originale contiene oltre 5.000.000 di caratteri. Tuttavia, ho deciso di utilizzare solo i primi 50_000 caratteri per ridurre i tempi di addestramento e permettermi di sperimentare più velocemente diverse configurazioni del modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Shakespeare from local file...\n",
      "Raw characters: 5376400\n",
      "Characters used: 50000\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "print(\"Loading Shakespeare from local file...\")\n",
    "\n",
    "file_path = \"shakespeare.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Raw characters:\", len(text))\n",
    "\n",
    "text = re.sub(r'\\s+', ' ', text)\n",
    "text = text[:50_000]\n",
    "\n",
    "print(\"Characters used:\", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Costruzione del vocabolario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inizialmente, ho individuato tutti i caratteri unici presenti nel corpus e questo insieme di caratteri costituirà il vocabolario del modello. \n",
    "\n",
    "Successivamente, si costruisce una corrispondenza tra ogni carattere e un numero intero e si crea anche la mappatura inversa, da numero a carattere.\n",
    "\n",
    "Infine, si sostituisce ogni carattere con il suo indice corrispondente nel vocabolario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: 71\n",
      "Sample: [' ', '!', '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?']...\n",
      "Encoded text shape: (50000,)\n",
      "First 20 encoded values: [ 0  8  0 25 59 56 54  0 47 42 50 59 46 60 61  0 44 59 46 42]\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Unique characters: {vocab_size}\\nSample: {chars[:20]}...\")\n",
    "\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = dict(enumerate(chars))\n",
    "\n",
    "encoded = np.array([char_to_idx[c] for c in text])\n",
    "print(f\"Encoded text shape: {encoded.shape}\")\n",
    "print(f\"First 20 encoded values: {encoded[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora, consideriamo il testo codificato (cioè la sequenza di numeri ottenuta), e lo dividiamo in sequenze di lunghezza fissa, definite dal parametro `SEQ_LENGTH`.\n",
    "\n",
    "`x` è una sequenza di 100 caratteri (convertiti in numeri)\n",
    "\n",
    "`y` è la stessa sequenza, spostata di una posizione in avanti.\n",
    "\n",
    "Il DataLoader viene utilizzato per organizzare questi esempi in batch, gruppi di `64` sequenze alla volta.  Inoltre, ho aggiunto anche l’opzione `shuffle=True`.\n",
    "L'insieme ordinato e pronto di sequenze verrà dato in input al modello `LSTM` nella fase di training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences: 49900 | DataLoader ready\n"
     ]
    }
   ],
   "source": [
    "SEQ_LENGTH = 100\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx:idx+self.seq_len], dtype=torch.long)\n",
    "        y = torch.tensor(self.data[idx+1:idx+self.seq_len+1], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "dataset = CharDataset(encoded, SEQ_LENGTH)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "print(f\"Total sequences: {len(dataset)} | DataLoader ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architettura del modello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il modello è composto da tre parti principali:\n",
    "\n",
    "1. Embedding layer `(nn.Embedding)`\n",
    "\n",
    "Il layer di embedding trasforma ogni indice in un vettore denso di dimensione `embed_size` (in questo caso `256`). Questo passaggio serve a fornire al modello una rappresentazione più significativa dei caratteri.\n",
    "\n",
    "2. LSTM multilayer `(nn.LSTM)`\n",
    "\n",
    "`hidden_size = 512` è la dimensione dello stato nascosto\n",
    "\n",
    "`num_layers = 3` LSTM è composta da 3 strati sovrapposti\n",
    "\n",
    "`dropout = 0.2` spegne casualmente il 20% delle connessioni tra gli strati \n",
    "\n",
    "Ho aggiunto anche la possibilità di usare una LSTM bidirezionale, però in questo caso è disattivata perché grazie alla memoria interna, il modello è abbastanza in grado di cogliere dipendenze tra caratteri lontani all’interno del testo (anche punteggiature e strutture sintattiche).\n",
    "\n",
    "3. Fully connected layer `(nn.Linear)`\n",
    "\n",
    "Ogni valore di questo vettore finale rappresenta uno score per ogni possibile carattere successivo. Durante il training, questi score verranno confrontati con il carattere reale usando la funzione di loss.\n",
    "\n",
    "Utilizzo `CrossEntropyLoss` come funzione di perdita e `Adam` come ottimizzatore, con `learning rate = 0.001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CharLSTM(\n",
      "  (embed): Embedding(71, 256)\n",
      "  (lstm): LSTM(256, 512, num_layers=3, batch_first=True, dropout=0.2)\n",
      "  (fc): Linear(in_features=512, out_features=71, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=256, hidden_size=512, num_layers=3, dropout=0.2, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_size, hidden_size, num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers>1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size * (2 if bidirectional else 1), vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embed(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CharLSTM(vocab_size, hidden_size=512, num_layers=3, dropout=0.2, bidirectional=False).to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addestramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "  Batch 0/779 | Loss: 4.2701\n",
      "  Batch 100/779 | Loss: 2.3172\n",
      "  Batch 200/779 | Loss: 1.9220\n",
      "  Batch 300/779 | Loss: 1.6269\n",
      "  Batch 400/779 | Loss: 1.4872\n",
      "  Batch 500/779 | Loss: 1.3491\n",
      "  Batch 600/779 | Loss: 1.2154\n",
      "  Batch 700/779 | Loss: 1.0873\n",
      "=== Epoch 1/5 finished | Avg Loss: 1.6571\n",
      "  Batch 0/779 | Loss: 0.9664\n",
      "  Batch 100/779 | Loss: 0.8310\n",
      "  Batch 200/779 | Loss: 0.7166\n",
      "  Batch 300/779 | Loss: 0.6037\n",
      "  Batch 400/779 | Loss: 0.5063\n",
      "  Batch 500/779 | Loss: 0.4632\n",
      "  Batch 600/779 | Loss: 0.4121\n",
      "  Batch 700/779 | Loss: 0.3642\n",
      "=== Epoch 2/5 finished | Avg Loss: 0.5691\n",
      "  Batch 0/779 | Loss: 0.3265\n",
      "  Batch 100/779 | Loss: 0.3119\n",
      "  Batch 200/779 | Loss: 0.2986\n",
      "  Batch 300/779 | Loss: 0.2852\n",
      "  Batch 400/779 | Loss: 0.2684\n",
      "  Batch 500/779 | Loss: 0.2733\n",
      "  Batch 600/779 | Loss: 0.2674\n",
      "  Batch 700/779 | Loss: 0.2647\n",
      "=== Epoch 3/5 finished | Avg Loss: 0.2872\n",
      "  Batch 0/779 | Loss: 0.2382\n",
      "  Batch 100/779 | Loss: 0.2417\n",
      "  Batch 200/779 | Loss: 0.2268\n",
      "  Batch 300/779 | Loss: 0.2490\n",
      "  Batch 400/779 | Loss: 0.2392\n",
      "  Batch 500/779 | Loss: 0.2178\n",
      "  Batch 600/779 | Loss: 0.2202\n",
      "  Batch 700/779 | Loss: 0.2143\n",
      "=== Epoch 4/5 finished | Avg Loss: 0.2329\n",
      "  Batch 0/779 | Loss: 0.2007\n",
      "  Batch 100/779 | Loss: 0.2260\n",
      "  Batch 200/779 | Loss: 0.2078\n",
      "  Batch 300/779 | Loss: 0.2184\n",
      "  Batch 400/779 | Loss: 0.2097\n",
      "  Batch 500/779 | Loss: 0.2064\n",
      "  Batch 600/779 | Loss: 0.2039\n",
      "  Batch 700/779 | Loss: 0.1931\n",
      "=== Epoch 5/5 finished | Avg Loss: 0.2079\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5 \n",
    "print(\"Training model...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    hidden = None\n",
    "\n",
    "    for i, (X_batch, y_batch) in enumerate(loader):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs, hidden = model(X_batch, hidden)\n",
    "\n",
    "        if hidden is not None:\n",
    "            hidden = (hidden[0].detach(), hidden[1].detach())\n",
    "\n",
    "        loss = criterion(outputs.reshape(-1, vocab_size), y_batch.reshape(-1))\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"  Batch {i}/{len(loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"=== Epoch {epoch+1}/{EPOCHS} finished | Avg Loss: {total_loss/len(loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generazione del testo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione prende in input:\n",
    "\n",
    "`start_text`: una sequenza iniziale di caratteri da cui partire.\n",
    "\n",
    "`length`: il numero di caratteri che vogliamo generare.\n",
    "\n",
    "`temperature`: un parametro che regola il randomness (creatività) del testo.\n",
    "\n",
    "La `temperature` viene applicata ai `logits`, dove valori più bassi rendono le predizioni più ripetitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_text=\"To be \", length=500, temperature=1.0):\n",
    "    model.eval()\n",
    "    input_seq = torch.tensor([char_to_idx[c] for c in start_text], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    hidden = None\n",
    "    result = start_text\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            out, hidden = model(input_seq, hidden)\n",
    "            logits = out[:, -1, :] / temperature\n",
    "            probs = torch.softmax(logits, dim=1).cpu().numpy().squeeze()\n",
    "            next_idx = np.random.choice(len(probs), p=probs)\n",
    "            result += idx_to_char[next_idx]\n",
    "            input_seq = torch.tensor([[next_idx]], dtype=torch.long).to(device)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione perplexity serve solo come strumento di valutazione aggiuntivo per quantificare numericamente quanto il modello sia bravo a prevedere i caratteri, senza dover leggere manualmente il testo generato. \n",
    "\n",
    "Prende in input:\n",
    "`text_sample`: un frammento di testo da valutare.\n",
    "\n",
    "Pertanto, per valori bassi, vuol dire che il modello è sicuro e il testo è facile da prevedere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(model, text_sample):\n",
    "    model.eval()\n",
    "    indices = torch.tensor([char_to_idx[c] for c in text_sample if c in char_to_idx]).to(device)\n",
    "    input_seq = indices[:-1].unsqueeze(0)\n",
    "    target = indices[1:]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs, _ = model(input_seq)\n",
    "        outputs = outputs.squeeze(0)\n",
    "        loss = nn.functional.cross_entropy(outputs, target)\n",
    "    return torch.exp(loss).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esempi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To be the painter and hath stelled, Thy beauty’s form in table of my heart, My body is the frame wherein ’tis held, And perspective it is best painter’s art. For through the painter must you see his skill, To find where your true image pictured lies, Which in my bosom’s shop is hanging still, That hath his windows glazed with thine eyes: Now see what good turns eyes for eyes have done, Mine eyes have drawn thy shape, and thine for me Are windows to my breast, where-through the sun Delights to peep, to'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, \"To be \", 500, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To be the basest clouds to ride, With ugly rack on his celestial face, And from the forlorn world his visage hide Stealing unseen to west with this disgrace: Even so my sun one early morn did shine, With all triumphant splendour on my brow, But out alack, he was but one hour mine, The region cloud hath masked him from me now. Yet him for this, my love no whit disdaineth, Suns of the world may stain, when heaven’s sun staineth. 34 Why didst thou promise such a beauteous day, And make me travel forth wi'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, \"To be \", 500, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To be or all the all of me. 32 If thou survive my well-contented day, When that churl death my bones with dust shall cover And shalt by fortune once more re-survey These poor rude lines of thy deceased lover: Compare them with the bett’ring of the time, And though they be outstripped by every pen, Reserve them for my love, not for their rhyme, Exceeded by the height of happier men. O then vouchsafe me but this loving thought, ’Had my friend’s Muse grown with this growing age, A dearer birth than this his'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, \"To be or \", 500, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial intelligence will rule the world’s due, by the grave and thee. 2 When forty winters shall besiege thy brow, And dig deep trenches in thy beauty’s field, Thy youth’s proud livery so gazed on now, Will be a tattered weed of small worth held: Then being asked, where all thy beauty lies, Where all the treasure of thy lusty days; '"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, \"Artificial intelligence will rule the \", 300, temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of the generated text: 1.09\n"
     ]
    }
   ],
   "source": [
    "sample_text = generate_text(model, start_text=\"To be \", length=200, temperature=0.5)\n",
    "\n",
    "pp = perplexity(model, sample_text)\n",
    "print(f\"Perplexity of the generated text: {pp:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydata-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
